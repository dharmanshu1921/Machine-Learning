{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aW18oeF40Xds"
   },
   "source": [
    "\n",
    "# Pipelined Processing - Simplify the Workflow\n",
    "\n",
    "A typical machine learning task generally involves data preparation to varying degrees. We won't get into the wide array of activities which make up data preparation here, but there are many. Such tasks are known for taking up a large proportion of time spent on any given machine learning task.\n",
    "\n",
    "After a dataset is cleaned up from a potential initial state of massive disarray, there are still several less-intensive yet no less-important transformative data preprocessing steps such as **feature extraction, feature scaling ** and **dimensionality reduction** to name just a few.\n",
    "\n",
    "Maybe preprocessing requires only one of these tansformations, such as some form of scaling. \n",
    "But maybe we need to string a number of transformations together, and ultimately finish off with an estimator of some sort. \n",
    "This is where Scikit-learn Pipelines can be helpful.\n",
    "\n",
    "Scikit-learn's Pipeline class is designed as a manageable way to apply a series of data transformations followed by the application of an estimator. \n",
    "\n",
    "In fact, that's really all it is:\n",
    "\n",
    "Pipeline of transforms with a final estimator.\n",
    "\n",
    "This simple tool is useful for:\n",
    "\n",
    "Convenience in creating a coherent and easy-to-understand workflow\n",
    "Enforcing workflow implementation and the desired order of step applications\n",
    "Reproducibility\n",
    "Value in persistence of entire pipeline objects (goes to reproducibility and convenience)\n",
    "So let's have a quick look at Pipelines. Specifically, here is what we will do.\n",
    "\n",
    "Build 3 pipelines, each with a different estimator (classification algorithm), using default hyperparameters:\n",
    "\n",
    "**Logisitic Regression **\n",
    "\n",
    "**Support Vector Machine **\n",
    "\n",
    "**Decision Tree **\n",
    "\n",
    "To demonstrate pipeline transforms, will perform:\n",
    "\n",
    "**> feature scaling **\n",
    "\n",
    "**> dimensionality reduction using PCA to project 4 dimensional data onto 2 dimensional space **\n",
    "\n",
    "**> then we will be fitting to our final estimators. **\n",
    "\n",
    "Afterwards, and almost completely unrelated, in order to make this a little more like a full-fledged workflow (it still isn't, but closer), we will:\n",
    "\n",
    "**> Followup with scoring test data\n",
    "> Compare pipeline model accuracies and identify the \"best\" model, meaning that which has the highest accuracy on our test data. **\n",
    "\n",
    "Persist (save to file) the entire pipeline of the \"best\" model generated. \n",
    "\n",
    "Given that we will use default hyperparameters, this likely won't result in the most accurate possible models, but it will provide a sense of how to use simple pipelines. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7RpYuuxS0RVN"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HcHQuTct0pAp"
   },
   "outputs": [],
   "source": [
    "# Load and split the data\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "VCJp921t0x-7",
    "outputId": "24ef5678-0183-4b21-f770-a1e82d9be2c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy SS_PCA_LR : 0.933 \n",
      "\n",
      "\n",
      " Accuracy SS_PCA_LR : 0.933 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Construct some pipelines\n",
    "pipe_ss_pca_lr = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('pca', PCA(n_components=2)),\n",
    "\t\t\t('clf', LogisticRegression(random_state=42))])\n",
    "\n",
    "\n",
    "\n",
    "pipe_ss_pca_lr.fit(X_train, y_train)\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Test...either this way....\n",
    "pred = pipe_ss_pca_lr.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy SS_PCA_LR : %.3f \\n' % accuracy_score(y_test, pred)) \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Or more convenient way...by using score method of the Pipeline class\n",
    "score = pipe_ss_pca_lr.score(X_test, y_test)\n",
    "print('\\n Accuracy SS_PCA_LR : %.3f \\n' %score) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOYabx4jzn25"
   },
   "source": [
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Spot Check Many Such Pipelined Estimators/Models\n",
    "\n",
    "### Now Lets Create a Pipeline of Many Such Classifiers all with their Own Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CeHbM4-m5KAg"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5Lez005zHIS"
   },
   "outputs": [],
   "source": [
    "# Construct a number of pipelines\n",
    "# Each Pipeline is associated with a different estimator with their own data preprocessing pipeline\n",
    "# In this example, all the estimators use the same preprocessing steps - standardize data and dimension reduction with PCA, before fitting the model.\n",
    "\n",
    "# Pipeline for estimator: Linear Regression\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('pca', PCA(n_components=2)),\n",
    "\t\t\t('clf', LogisticRegression(random_state=42))])\n",
    "\n",
    "# Pipeline for estimator: Support Vector Machine\n",
    "pipe_svm = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('pca', PCA(n_components=2)),\n",
    "\t\t\t('clf', svm.SVC(random_state=42))])\n",
    "\n",
    "# Pipeline for estimator: Decision Tree\n",
    "pipe_dt = Pipeline([('scl', StandardScaler()),\n",
    "\t\t\t('pca', PCA(n_components=2)),\n",
    "\t\t\t('clf', tree.DecisionTreeClassifier(random_state=42))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1KvMBGZ04bU"
   },
   "source": [
    "### Create a List of Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LlghCdTt03vu"
   },
   "outputs": [],
   "source": [
    "# List of pipelines for ease of iteration\n",
    "pipelines = [pipe_lr, pipe_svm, pipe_dt]\n",
    "\n",
    "# Dictionary of pipelines and classifier types for ease of reference\n",
    "pipe_dict = {0: 'Logistic Regression', 1: 'Support Vector Machine', 2: 'Decision Tree'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbLccT_F1FgC"
   },
   "source": [
    "### Iteratively Train all the Pipelines/Models and Test their Performances on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "_FCexQEV1HkB",
    "outputId": "2b34893a-f0fd-4baf-c584-918777d2893c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression pipeline test accuracy: 0.933 \n",
      "\n",
      "\n",
      " Support Vector Machine pipeline test accuracy: 0.900 \n",
      "\n",
      "\n",
      " Decision Tree pipeline test accuracy: 0.867 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit the pipelines\n",
    "for pipe in pipelines:\n",
    "\tpipe.fit(X_train, y_train)\n",
    "\n",
    "# Compare accuracies\n",
    "for idx, pipe in enumerate(pipelines):\n",
    "\tprint('\\n %s pipeline test accuracy: %.3f \\n' % (pipe_dict[idx], pipe.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTp_26Fi1TdE"
   },
   "source": [
    "### Find the Best Model by Comparing Their Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "ZzfmvN491TL2",
    "outputId": "43ce2e93-ad54-48ba-8336-71cf7c925f15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Classifier with best accuracy: Logistic Regression \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify the most accurate model on test data\n",
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_pipe = ''\n",
    "\n",
    "for idx, val in enumerate(pipelines):\n",
    "\tif val.score(X_test, y_test) > best_acc:\n",
    "\t\tbest_acc = val.score(X_test, y_test)\n",
    "\t\tbest_pipe = val\n",
    "\t\tbest_clf = idx\n",
    "print('\\n Classifier with best accuracy: %s \\n' % pipe_dict[best_clf])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d1H1O2rY1coA"
   },
   "source": [
    "#Model Persistence - Serialize the Best Model Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O-UmwzccwiJJ"
   },
   "source": [
    "We have two options\n",
    "\n",
    "1. pickle\n",
    "\n",
    "2. joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lQWfTvRI8ztP"
   },
   "source": [
    "# Option 1 :\n",
    "## Serialization with Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XptnYEV773GO"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "1WCsp95I7ubQ",
    "outputId": "efdac1e3-1543-4392-baa8-18fddae88cd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Serialized pipelined model to a disk file.... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = open('pipeline_model.pkl', 'wb') # open a file in write mode - binary file\n",
    "pickle.dump(best_pipe, output)\n",
    "output.close()\n",
    "print('\\n Serialized pipelined model to a disk file.... \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WrbAsBRZ_vqJ"
   },
   "source": [
    "### At some other time you may read the model from the disk and use it for prediction....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "dzU00jzj7zIT",
    "outputId": "0cf8a6c0-fe0e-46c5-eae8-c0853047a643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))])\n",
      "\n",
      " Model output [0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pkl_file = open('pipeline_model.pkl', 'rb')  # open the file in read mode - binary file\n",
    "model2 = pickle.load(pkl_file)\n",
    "\n",
    "print(model2)\n",
    "print('\\n Model output %s \\n' % model2.predict([[5, 3.5, 2, 1]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lo8sW7QHxOnp"
   },
   "source": [
    "# Option 2 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWfXDcYM9dA0"
   },
   "source": [
    "## Serialize with Joblib of Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TmSIBExx5O9O"
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "cRC3XUEt1U3J",
    "outputId": "13cfdc29-06f0-491e-8e61-986d19e2f996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Serialized pipelined model to a disk file.... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save pipeline to file\n",
    "joblib.dump(best_pipe, 'best_pipeline.jbl', compress=1)\n",
    "\n",
    "print('\\n Serialized pipelined model to a disk file.... \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpTLZ50K9x4b"
   },
   "source": [
    "### At some other time you may read the model from the disk and use it for prediction....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "iroUQ-lr8Kh5",
    "outputId": "2f880461-4e06-461f-d74f-ee42d3e12fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))])\n",
      "\n",
      " Model output [0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load saved model from the file\n",
    "model1= joblib.load('best_pipeline.jbl')\n",
    "\n",
    "print(model1)\n",
    "\n",
    "print('\\n Model output %s \\n' % model1.predict([[5, 3.5, 2, 1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSRCbO4i2fy2"
   },
   "source": [
    "## What after this?\n",
    "This is a simple implementation of Scikit-learn pipelines.\n",
    "In this particular case, our logistic regression-based pipeline with default parameters scored the highest accuracy.\n",
    "\n",
    "However, these results likely don't represent our best efforts. \n",
    "What if we did want to test a series of different hyperparameters? \n",
    "\n",
    "Can we use grid search? \n",
    "Can we incorporate automated methods for tuning these hyperparameters? \n",
    "What about using cross-validation?\n",
    "\n",
    "Try those...we have already learnt these concepts in our previous lectures."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
